{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from datasets import Dataset # Hugging Face Datasets\n",
    "import math\n",
    "import re # 正規表現によるパース用\n",
    "\n",
    "# --- 基本設定 ---\n",
    "# Generatorモデル (LoRAでファインチューニングする対象)\n",
    "generator_model_name = \"cyberagent/open-calm-7b\" # 例: 日本語モデル\n",
    "# Predictorモデル (IFDスコア算出用、固定)\n",
    "predictor_model_name = \"stabilityai/japanese-stablelm-instruct-gamma-7b\" # 例: 一問一答に強いとされるモデル\n",
    "\n",
    "# PEFT (LoRA) 設定\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# QLoRAを使う場合 (4bit量子化)\n",
    "use_qlora = True # Trueにすると4bit量子化を使用\n",
    "if use_qlora:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, # または torch.float16\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "else:\n",
    "    bnb_config = None\n",
    "\n",
    "# PPO設定\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=generator_model_name, # TRL内部での参照用\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=16,          # 1ステップで処理する文脈の数\n",
    "    mini_batch_size=4,      # PPOのミニバッチサイズ\n",
    "    ppo_epochs=4,           # 1回のデータ収集でPPOの更新を行うエポック数\n",
    "    log_with=\"wandb\",       # wandbなどのロギングツールを指定可能 (任意)\n",
    "    # gradient_accumulation_steps=1,\n",
    "    # early_stopping=False,\n",
    "    # target_kl=0.1,        # KLダイバージェンスの目標値\n",
    "    # kl_penalty=\"kl\",\n",
    "    # seed=42,\n",
    "    # init_kl_coef=0.2,\n",
    "    # adap_kl_ctrl=True,\n",
    ")\n",
    "\n",
    "# その他設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_context_length = 512   # Generatorに入力する文脈の最大長\n",
    "max_generation_length = 128 # Generatorが生成する「質問＋回答」の最大長\n",
    "num_epochs = 10            # 強化学習の総エポック数\n",
    "dataset_path = \"path/to/your/1000_documents.jsonl\" # データセットのパス (仮)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generatorモデルのロード ---\n",
    "print(f\"Loading generator tokenizer: {generator_model_name}\")\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(generator_model_name)\n",
    "if generator_tokenizer.pad_token is None:\n",
    "    generator_tokenizer.pad_token = generator_tokenizer.eos_token # PADトークンがない場合はEOSトークンで代用\n",
    "\n",
    "print(f\"Loading generator model: {generator_model_name}\")\n",
    "if use_qlora:\n",
    "    generator_model = AutoModelForCausalLM.from_pretrained(\n",
    "        generator_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0} # GPU 0 にロード (環境に合わせて調整)\n",
    "    )\n",
    "    generator_model = prepare_model_for_kbit_training(generator_model)\n",
    "else:\n",
    "    generator_model = AutoModelForCausalLM.from_pretrained(\n",
    "        generator_model_name,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16, # 量子化しない場合の型\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "generator_model = get_peft_model(generator_model, lora_config)\n",
    "# generator_model.print_trainable_parameters() # 学習可能なパラメータ数を表示\n",
    "\n",
    "# TRLのPPOではAutoModelForCausalLMWithValueHeadが必要\n",
    "# 既存のPEFTモデルからValueHead付きモデルを初期化\n",
    "generator_model_with_value_head = AutoModelForCausalLMWithValueHead.from_pretrained(generator_model)\n",
    "\n",
    "# 参照モデルの作成 (KLダイバージェンス計算用)\n",
    "# LoRAを適用する前の重みを持つモデルが理想だが、ここでは簡単のため同じモデルを複製\n",
    "# 実際には、学習開始時のgenerator_modelのコピーや、LoRA適用前のベースモデルを使う\n",
    "# ref_model = create_reference_model(generator_model_with_value_head)\n",
    "# または、学習初期の重みを保存しておき、それをロードする\n",
    "# ここでは、簡単のため、初期化されたもう一つのモデルをrefとして使う（TRLが内部で処理してくれる場合もある）\n",
    "# もしgenerator_model_with_value_headがPEFTモデルなら、それに対応した参照モデル作成が必要\n",
    "# TRLのドキュメントに従い、ref_modelはNoneにしてPPOTrainerに渡すと内部で作成される場合がある\n",
    "ref_model = None\n",
    "\n",
    "\n",
    "# --- Predictorモデルのロード ---\n",
    "print(f\"Loading predictor tokenizer: {predictor_model_name}\")\n",
    "predictor_tokenizer = AutoTokenizer.from_pretrained(predictor_model_name)\n",
    "if predictor_tokenizer.pad_token is None:\n",
    "    predictor_tokenizer.pad_token = predictor_tokenizer.eos_token\n",
    "\n",
    "print(f\"Loading predictor model: {predictor_model_name}\")\n",
    "# Predictorは量子化なしでロードするか、別途設定\n",
    "predictor_model = AutoModelForCausalLM.from_pretrained(\n",
    "    predictor_model_name,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    device_map=\"auto\" # 環境に合わせて調整\n",
    ")\n",
    "predictor_model.eval() # 評価モード\n",
    "\n",
    "print(f\"Generator model on: {generator_model_with_value_head.device}, Predictor model on: {predictor_model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_probability(model, tokenizer, text_sequence, context_sequence=None, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    if context_sequence:\n",
    "        # 文脈+質問 と 回答 の形式を想定\n",
    "        # トークナイザーによってはeos_tokenの扱いに注意\n",
    "        full_text = context_sequence + tokenizer.eos_token + text_sequence\n",
    "        input_ids = tokenizer.encode(full_text, return_tensors=\"pt\", truncation=True, max_length=tokenizer.model_max_length).to(device)\n",
    "        \n",
    "        context_plus_instruction_text = context_sequence # 論文ではQ (Instruction, [Input])\n",
    "        context_plus_instruction_ids = tokenizer.encode(context_plus_instruction_text + tokenizer.eos_token, return_tensors=\"pt\", truncation=True, max_length=tokenizer.model_max_length).to(device)\n",
    "        context_plus_instruction_length = context_plus_instruction_ids.shape[1]\n",
    "        \n",
    "        labels = input_ids.clone()\n",
    "        # 文脈+質問部分の損失は計算しない\n",
    "        if input_ids.shape[1] > context_plus_instruction_length:\n",
    "             labels[:, :context_plus_instruction_length] = -100\n",
    "        else: # 回答が空か非常に短い場合など\n",
    "             # この場合、有効なラベルがないので損失は0またはエラー。IFD計算には不適切。\n",
    "             print(f\"Warning: Answer part is empty or too short after tokenization for log_prob. Full text len: {input_ids.shape[1]}, Context+Instruction len: {context_plus_instruction_length}\")\n",
    "             return 1e9 # 非常に大きな損失（IFDスコアが高くなるように）\n",
    "    else:\n",
    "        # 回答Aのみ\n",
    "        input_ids = tokenizer.encode(text_sequence, return_tensors=\"pt\", truncation=True, max_length=tokenizer.model_max_length).to(device)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "    if input_ids.shape[1] == 0 or (context_sequence and input_ids.shape[1] <= context_plus_instruction_length): # 入力が空、またはラベルが全て-100になるケース\n",
    "        print(f\"Warning: No valid tokens to calculate loss. Input shape: {input_ids.shape}\")\n",
    "        return 1e9 # 非常に大きな損失\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        log_prob_score = outputs.loss.item()\n",
    "\n",
    "    if math.isnan(log_prob_score) or math.isinf(log_prob_score):\n",
    "        print(f\"Warning: Log probability score is NaN or Inf. Returning a large loss value.\")\n",
    "        return 1e9\n",
    "    return log_prob_score\n",
    "\n",
    "def calculate_normalized_ifd_reward(predictor_model, predictor_tokenizer, context_plus_question, answer, device=\"cpu\"):\n",
    "    \"\"\" 単一の (文脈+質問, 回答) ペアに対する正規化IFD報酬を計算 \"\"\"\n",
    "    if not context_plus_question or not answer:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "\n",
    "    s_A_given_Q = calculate_log_probability(predictor_model, predictor_tokenizer, answer, context_sequence=context_plus_question, device=device)\n",
    "    s_A = calculate_log_probability(predictor_model, predictor_tokenizer, answer, context_sequence=None, device=device)\n",
    "\n",
    "    if s_A == 0 or abs(s_A) < 1e-9: # ゼロ除算または非常に小さい値による不安定化を避ける\n",
    "        if s_A_given_Q > 1e-9 : # s_Aがほぼ0でs_A_given_Qが意味のある値ならIFDは発散\n",
    "            ifd_score = float('inf')\n",
    "        else: # 両方ほぼ0なら、IFD=1 (中立) または別の扱いに\n",
    "            ifd_score = 1.0\n",
    "    else:\n",
    "        ifd_score = s_A_given_Q / s_A\n",
    "    \n",
    "    if math.isinf(ifd_score) or ifd_score > 1e9: # 大きすぎる値をクリップ\n",
    "        normalized_score = 0.0\n",
    "    elif ifd_score < 0: # 損失が負になる異常ケース\n",
    "        normalized_score = 1.0 # IFDが負なら報酬最大 (要検討)\n",
    "    else:\n",
    "        normalized_score = 1.0 / (1.0 + ifd_score)\n",
    "    \n",
    "    return torch.tensor(normalized_score, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- データセットの準備 ---\n",
    "# ここでは、テキストファイルの各行が1つのドキュメント(文脈)であると仮定\n",
    "# 実際には、Hugging Face DatasetsライブラリでJSONLなどを読み込むのが良い\n",
    "try:\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        contexts = [line.strip() for line in f if line.strip()][:100] # テスト用に最初の100件\n",
    "    # TRLのPPOTrainerはHugging Face Datasets形式のデータセットを期待することがある\n",
    "    # context_dataset_dict = {\"query\": [generator_tokenizer.encode(f\"以下の文脈から一問一答を作成してください。\\n文脈: {ctx[:max_context_length]}\\n質問:\", return_tensors=\"pt\").to(device).squeeze(0) for ctx in contexts]}\n",
    "    # context_hf_dataset = Dataset.from_dict(context_dataset_dict)\n",
    "    # query_tensors = [generator_tokenizer(f\"以下の文脈から一問一答を作成してください。\\n文脈: {ctx[:max_context_length]}\\n質問:\", return_tensors=\"pt\", truncation=True, max_length=max_context_length).input_ids.squeeze(0).to(device) for ctx in contexts]\n",
    "\n",
    "    # PPOTrainerの初期化時にデータセットは渡さない（動的に生成するため）\n",
    "    # トークナイザーはgeneratorのものを使用\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config=ppo_config,\n",
    "        model=generator_model_with_value_head,\n",
    "        ref_model=ref_model, # create_reference_model(generator_model_with_value_head) または None\n",
    "        tokenizer=generator_tokenizer,\n",
    "        # dataset=context_hf_dataset, # データセットは後で与える\n",
    "        # data_collator=None\n",
    "    )\n",
    "    print(\"PPOTrainer initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing dataset or PPO trainer: {e}\")\n",
    "    # 適切なエラー処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 強化学習ループ ---\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1, # 生成を停止しない\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": generator_tokenizer.eos_token_id, # pad_token_id を eos_token_id に設定\n",
    "    \"max_new_tokens\": max_generation_length,\n",
    "    # \"eos_token_id\": -1 # 明示的にEOSで止めない場合 (PPOの挙動に影響する可能性)\n",
    "}\n",
    "\n",
    "# データローダーの代わり (簡単のため)\n",
    "def get_context_batch(contexts, batch_size, tokenizer, max_len, device):\n",
    "    for i in range(0, len(contexts), batch_size):\n",
    "        batch = contexts[i:i+batch_size]\n",
    "        # Generatorへの入力プロンプトを作成\n",
    "        # このプロンプトエンジニアリングが重要\n",
    "        prompts = [f\"以下の文脈から主要な情報に関する質問とその端的な回答を一つ作成してください。\\n\\n文脈:\\n{ctx}\\n\\n質問:\" for ctx in batch]\n",
    "        \n",
    "        # query_tensors: generatorへの入力 (プロンプト)\n",
    "        # PPOTrainerの generate メソッドは tokenizedされたリストのリストを期待する\n",
    "        query_tensors = [tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len).to(device).squeeze(0) for prompt in prompts]\n",
    "        # query_texts = prompts # ログ用\n",
    "        yield query_tensors, prompts # トークン化されたプロンプトと元のプロンプトテキストを返す\n",
    "\n",
    "print(\"Starting PPO training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, (query_tensors_batch, prompt_texts_batch) in enumerate(get_context_batch(contexts, ppo_config.batch_size, generator_tokenizer, max_context_length, device)):\n",
    "        if not query_tensors_batch:\n",
    "            continue\n",
    "\n",
    "        print(f\"  Step {step+1}, Batch size: {len(query_tensors_batch)}\")\n",
    "\n",
    "        # 1. Generatorによる一問一答データの生成 (response_tensors には生成された「質問+回答」部分のみが入る)\n",
    "        # response_tensors = ppo_trainer.generate(query_tensors_batch, **generation_kwargs)\n",
    "        # generateはリストのリストを返す (各要素がTensor)\n",
    "        response_tensors_list = []\n",
    "        for query_tensor in query_tensors_batch:\n",
    "            # 各クエリテンソルに対して個別に生成\n",
    "            # TRLの generate メソッドは query (単一のTensor) を期待\n",
    "            response_tensor = ppo_trainer.generate(query_tensor, **generation_kwargs)\n",
    "            response_tensors_list.append(response_tensor.squeeze()) # バッチ次元を削除\n",
    "\n",
    "        # 生成されたテキストをデコード\n",
    "        # query_texts_batch: generatorへの入力プロンプトのリスト (デコード済み)\n",
    "        # response_texts_batch: generatorが生成した「質問+回答」のテキストのリスト\n",
    "        # input_texts_for_decode = [query_tensors_batch[i] for i in range(len(response_tensors_list))] # これだとプロンプト\n",
    "        \n",
    "        # response_tensors_list は生成された部分のみ\n",
    "        response_texts_batch = [generator_tokenizer.decode(r_tensor, skip_special_tokens=True) for r_tensor in response_tensors_list]\n",
    "\n",
    "        # 2. 生成テキストから「文脈+質問」と「回答」をパース\n",
    "        #    報酬計算のために、generatorへの入力(文脈)と、生成された質問、生成された回答が必要\n",
    "        #    predictorへの入力は「文脈＋生成された質問」\n",
    "        parsed_for_predictor = []\n",
    "        for i in range(len(prompt_texts_batch)):\n",
    "            original_context_prompt = prompt_texts_batch[i] # \"以下の文脈...質問:\"\n",
    "            generated_qa_text = response_texts_batch[i]     # Generatorが「質問:」の後に生成したテキスト\n",
    "\n",
    "            # \"質問:\" の後の実際の文脈部分を抽出 (プロンプトの構造に依存)\n",
    "            # ここはプロンプト設計と密接に関連\n",
    "            context_match = re.search(r\"文脈:\\n(.*?)\\n\\n質問:\", original_context_prompt, re.DOTALL)\n",
    "            if not context_match:\n",
    "                print(f\"    Warning: Could not parse context from prompt: {original_context_prompt}\")\n",
    "                parsed_for_predictor.append((\"\", \"\", \"\")) # (文脈+生成質問, 生成回答, 元の文脈プロンプト)\n",
    "                continue\n",
    "            \n",
    "            actual_context = context_match.group(1).strip()\n",
    "\n",
    "            # generated_qa_text から「生成された質問」と「生成された回答」を分離する\n",
    "            # 例: 生成テキストが \"これは質問ですか？\\n回答: はい、そうです。\" のようになっていると仮定\n",
    "            # このパース処理は非常に重要であり、実際の生成形式に合わせて頑健にする必要がある\n",
    "            # ここでは単純な改行と \"回答:\" で分割を試みる\n",
    "            parts = generated_qa_text.split(\"回答:\", 1)\n",
    "            if len(parts) == 2:\n",
    "                generated_question = parts[0].strip()\n",
    "                generated_answer = parts[1].strip()\n",
    "            else: # パース失敗\n",
    "                print(f\"    Warning: Could not parse Q/A from: {generated_qa_text}\")\n",
    "                generated_question = generated_qa_text # 全体を質問とみなすか、空にするか\n",
    "                generated_answer = \"\"\n",
    "\n",
    "            if not generated_question or not generated_answer:\n",
    "                print(f\"    Warning: Parsed question or answer is empty. Q: '{generated_question}', A: '{generated_answer}'\")\n",
    "                # 報酬0にするために空でないようにダミーを入れるか、後でフィルタリング\n",
    "                \n",
    "            context_plus_generated_question = actual_context + \"\\n質問: \" + generated_question # predictorへの入力\n",
    "            parsed_for_predictor.append((context_plus_generated_question, generated_answer, original_context_prompt))\n",
    "\n",
    "        # 3. IFDスコアを正規化した報酬の計算\n",
    "        rewards_list = []\n",
    "        valid_query_tensors = []\n",
    "        valid_response_tensors = []\n",
    "\n",
    "        for i, (ctx_q_for_pred, ans_for_pred, _) in enumerate(parsed_for_predictor):\n",
    "            if ans_for_pred: # 回答がパースできた場合のみ\n",
    "                reward = calculate_normalized_ifd_reward(predictor_model, predictor_tokenizer, ctx_q_for_pred, ans_for_pred, device)\n",
    "                rewards_list.append(reward)\n",
    "                valid_query_tensors.append(query_tensors_batch[i]) # 元のgeneratorへの入力プロンプト\n",
    "                valid_response_tensors.append(response_tensors_list[i]) # generatorが生成したQ+A部分\n",
    "            else:\n",
    "                # 質の低い生成やパース失敗の場合は低い報酬を与えるか、このサンプルを学習から除外\n",
    "                # ここでは学習から除外するアプローチ（validリストに追加しない）\n",
    "                print(f\"    Skipping sample due to empty parsed answer. Original generated text: {response_texts_batch[i]}\")\n",
    "        \n",
    "        if not rewards_list: # 有効な報酬が得られなかった場合はスキップ\n",
    "            print(\"    No valid rewards generated for this batch, skipping PPO step.\")\n",
    "            continue\n",
    "            \n",
    "        rewards_tensor = torch.stack(rewards_list)\n",
    "\n",
    "        # 4. PPOトレーナーで学習ステップを実行\n",
    "        # query_tensors は generator への入力プロンプト\n",
    "        # response_tensors は generator が生成した部分 (質問+回答)\n",
    "        # rewards は各 (query, response) ペアに対する報酬\n",
    "        # TRLのPPOTrainerはリストのリストではなく、単一のリストを期待することが多い\n",
    "        # また、各要素は1D Tensor\n",
    "        \n",
    "        # query_tensors_for_step = [qt.squeeze(0) for qt in valid_query_tensors] # (seq_len)\n",
    "        # response_tensors_for_step = [rt.squeeze(0) for rt in valid_response_tensors] # (gen_len)\n",
    "\n",
    "        # stats = ppo_trainer.step(query_tensors_for_step, response_tensors_for_step, rewards_tensor)\n",
    "        stats = ppo_trainer.step(valid_query_tensors, valid_response_tensors, rewards_tensor)\n",
    "\n",
    "\n",
    "        # ログ出力 (任意)\n",
    "        log_output = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"step\": step + 1,\n",
    "            \"mean_reward\": rewards_tensor.mean().item() if rewards_tensor.numel() > 0 else 0,\n",
    "            \"ppo/loss/policy\": stats.get(\"ppo/loss/policy\"),\n",
    "            \"ppo/loss/value\": stats.get(\"ppo/loss/value\"),\n",
    "        }\n",
    "        print(f\"    Log: {log_output}\")\n",
    "        if ppo_config.log_with == \"wandb\":\n",
    "            ppo_trainer.log_stats(stats, {\"query\": prompt_texts_batch[:len(valid_query_tensors)]}, rewards_tensor, {\"response\": [generator_tokenizer.decode(r, skip_special_tokens=True) for r in valid_response_tensors]})\n",
    "\n",
    "\n",
    "# 学習後のモデルの保存 (LoRAアダプタのみ保存)\n",
    "# generator_model_with_value_head.save_pretrained(\"path/to/your/final_generator_ppo_model\")\n",
    "# generator_tokenizer.save_pretrained(\"path/to/your/final_generator_ppo_model\")\n",
    "# PEFTモデルの保存方法を確認 (通常はアダプタのみ)\n",
    "ppo_trainer.model.save_pretrained(\"path/to/your/final_generator_ppo_lora_adapters\")\n",
    "generator_tokenizer.save_pretrained(\"path/to/your/final_generator_ppo_lora_adapters\")\n",
    "\n",
    "\n",
    "print(\"PPO training attempt finished.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
