import streamlit as st
import pandas as pd
import json
import sys
import re
from pathlib import Path

# --- è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰ç§»æ¤ã—ãŸæ­£è¦åŒ–é–¢æ•° ---
def normalize_answer(answer: str) -> str:
    """
    å›ç­”æ–‡å­—åˆ—ã‚’æ­£è¦åŒ–ã™ã‚‹ï¼ˆæœ«å°¾ã®å¥ç‚¹ãƒ»ãƒ”ãƒªã‚ªãƒ‰ã®å‰Šé™¤ã€ç©ºç™½é™¤å»ï¼‰ã€‚
    evaluate.pyã®ãƒ­ã‚¸ãƒƒã‚¯ã¨å®Œå…¨ã«ä¸€è‡´ã•ã›ã¦ã„ã¾ã™ã€‚
    """
    if not isinstance(answer, str):
        return ""
    normalized = answer.strip()
    normalized = re.sub(r"[ã€‚\.]+$", "", normalized)
    return normalized

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ + æ­£ã—ã„æ­£è§£ç‡è¨ˆç®—ï¼‰ ---
@st.cache_data
def load_all_data(results_dir: Path, judge_filepath: Path):
    """
    è©•ä¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€
    1ã¤ã®çµ±åˆã•ã‚ŒãŸDataFrameã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """
    try:
        # 1. è©•ä¾¡åŸºæº–ã¨ãªã‚‹judgeãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        with judge_filepath.open('r', encoding='utf-8') as f:
            records = [json.loads(line) for line in f]
        
        processed_records = []
        for r in records:
            qc = r.get("question_clarity", {})
            r["clarity_score"] = qc.get("score")
            r["clarity_reason"] = qc.get("reason")
            cg = r.get("context_grounding", {})
            r["grounding_score"] = cg.get("score")
            r["grounding_reason"] = cg.get("reason")
            processed_records.append(r)
        
        base_df = pd.DataFrame(processed_records)
        if 'id' not in base_df.columns:
            st.error(f"{judge_filepath.name}ã« 'id' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return pd.DataFrame(), []

        base_df['page_id'] = base_df['id'].apply(lambda x: x.rsplit('q', 1)[0] if isinstance(x, str) else None)
        
        # 'is_contained' (åŸºæº–3) ã®è¨ˆç®—
        context_map = base_df.groupby('page_id')['context'].first().to_dict()
        page_contexts = base_df['page_id'].map(context_map)
        results = [
            (ans in ctx if isinstance(ans, str) and isinstance(ctx, str) else False)
            for ans, ctx in zip(base_df['answer'], page_contexts)
        ]
        base_df['is_contained'] = results

        # 2. å„ãƒ¢ãƒ‡ãƒ«ã®prediction.jsonlã‚’èª­ã¿è¾¼ã‚“ã§ãƒãƒ¼ã‚¸
        model_names = []
        for model_dir in results_dir.iterdir():
            pred_file = model_dir / "prediction.jsonl"
            if model_dir.is_dir() and pred_file.exists():
                model_name = model_dir.name
                model_names.append(model_name)
                pred_df = pd.read_json(pred_file, lines=True, dtype={'id': str})
                pred_df = pred_df[['id', 'answer']].rename(columns={'answer': f'answer_{model_name}'})
                base_df = pd.merge(base_df, pred_df, on='id', how='left')

        # 3. å„ãƒ¢ãƒ‡ãƒ«ã®æ­£è§£/ä¸æ­£è§£ã‚’åˆ¤å®šï¼ˆæ­£è¦åŒ–ã‚’é©ç”¨ï¼‰
        correct_cols = []
        for model_name in model_names:
            correct_col = f'correct_{model_name}'
            answer_col = f'answer_{model_name}'
            # æ­£è¦åŒ–ã—ãŸä¸Šã§å›ç­”ã‚’æ¯”è¼ƒ
            base_df[correct_col] = base_df.apply(
                lambda row: normalize_answer(row.get('answer')) == normalize_answer(row.get(answer_col)),
                axis=1
            )
            correct_cols.append(correct_col)

        # 4. è¨­å•ã”ã¨ã®å¹³å‡æ­£è§£ç‡ã‚’è¨ˆç®—
        if correct_cols:
            base_df['avg_accuracy'] = base_df[correct_cols].mean(axis=1)
        else:
            base_df['avg_accuracy'] = 0.0
        
        # å…±é€šã®å‡¦ç†
        base_df['clarity_score'] = pd.to_numeric(base_df['clarity_score'], errors='coerce')
        base_df['grounding_score'] = pd.to_numeric(base_df['grounding_score'], errors='coerce')
        
        return base_df, sorted(model_names)
        
    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.exception(e)
        return pd.DataFrame(), []

# --- UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ ---
def display_score_badge(score, label):
    if pd.isna(score): color, text = "#808080", "N/A"
    elif score == 1: color, text = "#28a745", "Good"
    else: color, text = "#dc3545", "Bad"
    st.markdown(f'<span style="background-color: {color}; color: white; padding: 3px 10px; border-radius: 15px;">{label}: {text}</span>', unsafe_allow_html=True)

def display_boolean_badge(is_contained, label):
    if is_contained: color, text = "#28a745", "Contained"
    else: color, text = "#dc3545", "Not Contained"
    st.markdown(f'<span style="background-color: {color}; color: white; padding: 3px 10px; border-radius: 15px;">{label}: {text}</span>', unsafe_allow_html=True)

# --- å…¨ä½“ã‚µãƒãƒªãƒ¼è¡¨ç¤ºé–¢æ•° ---
def display_overall_summary(df, model_names):
    st.header("ğŸ“Š å…¨ä½“çµæœã‚µãƒãƒªãƒ¼")
    total_items = len(df)
    if total_items == 0: return
        
    st.metric(label="ç·ãƒ‡ãƒ¼ã‚¿æ•°", value=f"{total_items} ä»¶")
    st.divider()

    st.subheader("LLM-as-a-Judge è©•ä¾¡åŸºæº–")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("###### åŸºæº–1: è³ªå•ã®æ˜ç­æ€§ (Clarity)")
        clarity_good = df["clarity_score"].eq(1).sum()
        st.metric(label="Goodç‡", value=f"{(clarity_good / total_items) * 100:.1f} %")
    with col2:
        st.markdown("###### åŸºæº–2: æ–‡è„ˆã¸ã®æº–æ‹ åº¦ (Grounding)")
        grounding_good = df["grounding_score"].eq(1).sum()
        st.metric(label="Goodç‡", value=f"{(grounding_good / total_items) * 100:.1f} %")
    with col3:
        st.markdown("###### åŸºæº–3: å›ç­”ã®æ–‡è„ˆåŒ…å«ç‡ (Containment)")
        contained_good = df['is_contained'].sum()
        st.metric(label="åŒ…å«ç‡", value=f"{(contained_good / total_items) * 100:.1f} %")
    
    st.divider()

    st.header("ğŸ† ãƒ¢ãƒ‡ãƒ«åˆ¥ æ­£è§£ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
    if not model_names:
        st.warning("æ¯”è¼ƒå¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        accuracies = {model: (df[f'correct_{model}'].sum() / total_items) * 100 for model in model_names}
        acc_df = pd.DataFrame.from_dict(accuracies, orient='index', columns=['Accuracy (%)'])
        acc_df = acc_df.sort_values('Accuracy (%)', ascending=False)
        st.dataframe(acc_df.style.format("{:.2f}%").background_gradient(cmap='Greens'), use_container_width=True)

# --- ãƒšãƒ¼ã‚¸åˆ¥è©³ç´°è¡¨ç¤ºé–¢æ•° ---
def display_page_details(page_df, model_names):
    context = page_df.iloc[0]['context']
    st.header("ğŸ“„ ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ")
    st.text_area("Context", value=context, height=150, disabled=True)
    st.divider()

    st.header("å€‹åˆ¥ã®è©•ä¾¡çµæœãƒªã‚¹ãƒˆ")
    
    for _, item in page_df.iterrows():
        # Judgeã«ã‚ˆã‚‹è©•ä¾¡çµæœã®ã‚µãƒãƒªãƒ¼ãƒãƒƒã‚¸
        avg_acc = item.get('avg_accuracy', 0) * 100
        c_score, g_score, is_cont = item['clarity_score'], item['grounding_score'], item['is_contained']
        if c_score == 1 and g_score == 1 and is_cont: badge = f"âœ… All Passed, Avg Acc: {avg_acc:.1f}%"
        else:
            c_b = "ğŸ‘" if c_score == 1 else "ğŸ‘" if c_score == 0 else "â”"
            g_b = "ğŸ‘" if g_score == 1 else "ğŸ‘" if g_score == 0 else "â”"
            a_b = "ğŸ‘" if is_cont else "ğŸ‘"
            badge = f"Clarity:{c_b}, Grounding:{g_b}, Containment:{a_b}, Avg Acc:{avg_acc:.1f}%"

        with st.expander(f"**ID:** {item['id']}  â€”  **Judge Eval:** {badge}"):
            st.markdown("##### LLM-as-a-Judgeã«ã‚ˆã‚‹è©•ä¾¡")
            b1, b2, b3 = st.columns(3)
            with b1: display_score_badge(c_score, "Clarity")
            with b2: display_score_badge(g_score, "Grounding")
            with b3: display_boolean_badge(is_cont, "Containment")

            ### ### å¤‰æ›´ç®‡æ‰€: è©•ä¾¡ç†ç”±ã®è¡¨ç¤ºæ©Ÿèƒ½ã‚’è¿½åŠ  ### ###
            st.markdown("##### è©•ä¾¡ç†ç”±")
            r1, r2 = st.columns(2)
            with r1:
                st.info(f"**Clarity:** {item.get('clarity_reason', 'N/A')}")
            with r2:
                st.info(f"**Grounding:** {item.get('grounding_reason', 'N/A')}")
            ### ### ã“ã“ã¾ã§ ### ###
            
            st.markdown(f"**â“ Question:**\n> {item['question']}")
            st.markdown(f"**ğŸ’¡ Judge Answer:**")
            st.success(f"{item['answer']}")
            
            st.markdown("---")
            
            st.metric(label="å…¨ãƒ¢ãƒ‡ãƒ«å¹³å‡æ­£è§£ç‡", value=f"{avg_acc:.1f} %")

            st.markdown("##### å„ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã¨æ­£èª¤")
            for model_name in model_names:
                model_answer = item.get(f'answer_{model_name}', "N/A")
                is_correct = item.get(f'correct_{model_name}', False)
                icon = "âœ…" if is_correct else "âŒ"
                st.markdown(f"`{icon}` **{model_name}:** \n> {model_answer}")

# --- ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒšãƒ¼ã‚¸é€ã‚Šæ©Ÿèƒ½ãªã©ã‚’å¾©å…ƒï¼‰ ---
def main():
    st.set_page_config(layout="wide")
    st.title("ğŸ¤– LLM Judge & Model Comparison Viewer")

    try:
        results_dir = Path(sys.argv[1])
        judge_filepath = Path(sys.argv[2])
    except IndexError:
        st.error("ã‚¨ãƒ©ãƒ¼: 2ã¤ã®ãƒ‘ã‚¹ã‚’ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        st.info("ä¾‹: streamlit run app.py <çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹> <è©•ä¾¡ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹>")
        return
    
    st.sidebar.info(f"çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: \n`{results_dir}`")
    st.sidebar.info(f"è©•ä¾¡ãƒ•ã‚¡ã‚¤ãƒ«: \n`{judge_filepath}`")

    if not results_dir.is_dir() or not judge_filepath.is_file():
        st.error(f"ã‚¨ãƒ©ãƒ¼: æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª/ãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    df, model_names = load_all_data(results_dir, judge_filepath)
    if df.empty: return

    st.sidebar.header("è¡¨ç¤ºé¸æŠ")
    # page_idãŒNoneã®ã‚‚ã®ã‚’é™¤å¤–
    valid_page_ids = sorted([pid for pid in df['page_id'].unique() if pid is not None])
    page_id_list = ["Overall"] + valid_page_ids
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’åˆ©ç”¨ã—ãŸãƒšãƒ¼ã‚¸ç®¡ç†
    if 'page_index' not in st.session_state:
        st.session_state.page_index = 0

    col_prev, col_next = st.sidebar.columns(2)
    if col_prev.button("â¬…ï¸ å‰ã¸", use_container_width=True):
        st.session_state.page_index = max(0, st.session_state.page_index - 1)
        st.rerun()
    if col_next.button("æ¬¡ã¸ â¡ï¸", use_container_width=True):
        st.session_state.page_index = min(len(page_id_list) - 1, st.session_state.page_index + 1)
        st.rerun()

    selected_page_id = st.sidebar.selectbox(
        "ãƒšãƒ¼ã‚¸IDã¾ãŸã¯Overallã‚’é¸æŠ",
        options=page_id_list,
        index=st.session_state.page_index,
        key="page_selector"
    )
    # selectboxã®å¤‰æ›´ã‚’session_stateã«åæ˜ 
    if page_id_list.index(selected_page_id) != st.session_state.page_index:
        st.session_state.page_index = page_id_list.index(selected_page_id)
        st.rerun()

    st.sidebar.divider()
    st.sidebar.info(f"è¡¨ç¤ºä¸­: `{selected_page_id}`")
    
    if selected_page_id == "Overall":
        display_overall_summary(df, model_names)
    else:
        page_df = df[df['page_id'] == selected_page_id].copy()
        display_page_details(page_df, model_names)

if __name__ == "__main__":
    main()
